# ğŸŒ¸ observable & scalable iris classifier api

[![build](https://img.shields.io/badge/build-passing-brightgreen)](https://github.com/adhithyasash1/observable-k8s-pipeline/actions)
[![deployment](https://img.shields.io/badge/deployment-live-blue)](http://34.55.215.228)
[![kubernetes](https://img.shields.io/badge/kubernetes-gke-success)](https://cloud.google.com/kubernetes-engine)
[![observability](https://img.shields.io/badge/observability-enabled-yellowgreen)](https://cloud.google.com/trace)

this project builds and scales a machine learning inference api using fastapi and kubernetes. it supports multiple concurrent requests and includes observability features using google cloud logging and tracing.

---

rest of the readme follows as before. if you want custom icons or to match your repo's actual CI/CD status badges, i can fetch those too.

---

## ğŸ¯ objective

scale a basic iris classification api to handle concurrent load using kubernetes, and identify performance bottlenecks with open telemetry.

---

## ğŸ§± project structure

```
observable-k8s-pipeline/
â”œâ”€â”€ app/                   â†’ fastapi app + model
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ model.joblib
â”œâ”€â”€ k8s/                   â†’ kubernetes manifests
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ hpa.yaml
â”‚   â””â”€â”€ service.yaml
â”œâ”€â”€ Dockerfile             â†’ multi-stage docker build
â”œâ”€â”€ pyproject.toml         â†’ dependencies (via poetry)
â”œâ”€â”€ post.lua               â†’ wrk load test script
â”œâ”€â”€ wrk_results.txt        â†’ benchmarking output
â”œâ”€â”€ command_history.txt    â†’ full terminal command log
```

---

## âš™ï¸ how it works

* **fastapi** serves a `/predict` endpoint using a pre-trained sklearn model
* requests are traced using **open telemetry** and **cloud trace**
* logs are structured in json and sent to **cloud logging**
* deployed on **gke** with autoscaling enabled using **hpa**
* load testing is done using `wrk` with 100 concurrent users

---

## â˜ï¸ cloud setup steps

1. enabled apis: `cloud trace`, `cloud logging`, `artifact registry`, `gke`
2. created a gke cluster named `iris-scaling-cluster`
3. enabled workload identity to allow k8s service account to impersonate a gcp service account
4. created a gcp service account named `telemetry-access`
5. granted it:

   * `roles/logging.logWriter`
   * `roles/cloudtrace.agent`
   * `roles/iam.workloadIdentityUser`
6. annotated the k8s service account to use the gcp one:

   ```bash
   kubectl annotate serviceaccount telemetry-access \
     iam.gke.io/gcp-service-account=telemetry-access@$PROJECT_ID.iam.gserviceaccount.com
   ```

---

## ğŸ³ container build & deploy

* used poetry for dependency management
* built and pushed the image to artifact registry:

  ```bash
  docker build -t $IMAGE_PATH .
  docker push $IMAGE_PATH
  ```
* deployed with:

  ```bash
  kubectl apply -f k8s/deployment.yaml
  kubectl apply -f k8s/service.yaml
  kubectl apply -f k8s/hpa.yaml
  ```

---

## ğŸ“ˆ observability details

* spans created for every request and inference step
* latency captured and sent to cloud trace
* response headers include `x-process-time-ms`
* logs are formatted in json with trace ids

---

## ğŸ§ª load test summary (`wrk`)

* tested with:

  ```
  wrk -t4 -c100 -d30s --latency -s post.lua http://<external-ip>/predict
  ```
* results:

  * avg latency: \~697ms
  * requests/sec: \~141
  * total requests: 4,244
* full results in `wrk_results.txt`

---

## ğŸ” sample request

```bash
curl -X POST http://<external-ip>/predict \
  -H "content-type: application/json" \
  -d '{"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2}'
```

---

## ğŸ‘¤ author

R Sashi Adhithya /
(21F3000611) /
github: [@adhithyasash1](https://github.com/adhithyasash1)
